#chapter 3###################################################################
#lynda - R stats
#calculating frequencies ----------------------

groups<- c(rep('blue',3990),
           rep('red',4140),
           rep('orange',1890),
           rep('green',3770),
           rep('purple',855))

#create freq table
groups.t1<- table(groups)
groups.t1

#sort in decsending order, remember: sort for vectors, order for dfs
groups.t2<- sort(groups.t1,decreasing=TRUE)

#proportions and percentages
round(prop.table(groups.t2),2)

#CALCULATING DESCRIPTIVES-----------------------------

require('datasets')
cars<- cars

str(cars)

summary(cars$speed)
summary(cars)


install.packages('psych')

library('psych')
describe(cars)

#Using a single proportion: hypothesis test and confidence interval

#in 2012 MLB, nationals had best record in season: 98/162 wins
# .605. Is this significantly greater than 50%?

#Prop Test
#number outcomes out of number of opportunities
prop.test(98,162)

#p-value of .009 means its significant, reject null hypothesis

#one tailed test with 90% CI
prop.test(98,162,alt='greater',conf.level=.9)
#notice alt hypothese: true p is greater than 0.5

#SINGLE MEAN: HYPOTHESIS TEST AND CI--------------------

quakes<-quakes
head(quakes)

mag<-quakes$mag

t.test(mag,mu=4,alternative='greater')
#compare mean of sample to hypothesized value,
#probability that mu is the mean of the population


#Using a single categorical variable: one sample chi-square test
#if data includes categorical variable where ppl fall into diff groups
#this is the test to use (also called goodness of fit test)

h<- HairEyeColor

h

#get marginal freq for eye color
eyes<- margin.table(h,2)
eyes

#use pearson's chi squared test
#need one-dimensional goodness of fit test
#default test (assumes equal distribution)

chi1<- chisq.test(eyes)
chi1
#of course wouldn't match null hypothesis: which is equality
#match to population proportions

p=c(.41,.32,.15,.12)
chi2<- chisq.test(eyes,p=c(.41,.32,.15,.12))

chi2
#probability of getting observed findings if the null hyp
# is true, equals .09. Therefore this group does not differ
#significantly from the general population


#ROBUST STATISTICS FOR UNIVARIATE ANALYSES
#data doesn't fit assumptions of test
#procedures resistant to outliers/non-normality in dist

state<- state.area

hist(state)
boxplot(state)
boxplot.stats(state)

mean(state)      #not robust - i.e. if there are outliers they WILL skew the mean

median(state)    #more robust measure of center

mean(state,trim=.05)    #5% from each end (10% total)
#trimmed mean is much more robust

#robust methods for describing variation:
sd(state)       #NOT robust

mad(state)      #median absolute deviation

IQR(state)    #Interquartile range, different thing

#Challenge: calculate mean, SD, skewness, kurtosis
#for mpg, hp, qsec from mtcars

describe(mtcars[c(1,4,7)])[,c(3,4,11,12)]
library(psych)

#Chapter 4: Modifying Data############################################################################
#EXAMINING OUTLIERS------------------------------------------

 
#categorical outlier is one that constitutes <10%

OS<- read.csv('C:/R-3.2.3/Play/Ex_Files_RStats_EssT/Exercise Files/Ch04/04_01/OS.csv')

#options: combine into MISC category, or delete

OS.hi<- subset(OS,Proportion>0.1)
OS.hi

rivers<-rivers
hist(rivers)
boxplot.stats(rivers)
rivers.low<-rivers[rivers<1210]

#TRANSFORMING VARIABLES----------------

require('datasets')

hist(islands,breaks=16)
#boxplot shows data is way off normal dist

islands.z<- scale(islands) #standardizes to mean of 0 and sd of 1
islands.z

hist(islands.z,breaks=16)

attr(islands.z,'scaled:center')
#R saves meta data: old mean of data set before standardization
attr(islands.z,'scaled:scale')
#shows original SD

islands.z<- as.numeric(islands.z)

#LOG TRANSFORM - Great for dealing with outlier on high end

islands.ln<- log(islands) #natural log (base =e)
  #islands.log10<- log10(islands) #common log (base =10)

hist(islands.ln)
boxplot(islands.ln)

#NOTE: can't log a 0 value.
#Add 1 or some value to avoid undefined value logs when x =0
#x.ln<- log(x+1)

#SQUARING - FOR NEGATIVELY SKEWED VARIABLES

#RANKING - maintains order but nothing else

islands.rank1<- rank(islands)
#shows ties in the data
#ties.method = c('average','first','random','max','min')
islands.rank2<- rank(islands,ties.method='random')

#DICHOTOMIZING - use wisely and purposefully
#splitting dataset into high and low

#split at 1000 in this case (1,000,000 square miles)
continent<- ifelse(islands>1000,1,0)

#COMPUTING COMPOSITE VARIABLES ------

rn1<- rnorm(1000000)
hist(rn1)
summary(rn1)

rn2<- rnorm(1000000)

rn.mean<- (rn1+rn2)/2
#if vector same length, no need for loop like many languages
hist(rn.mean)

rn.prod <- rn1* rn2
hist(rn.prod)


#kurtosis comparisons
#how peaked, curved at bottom - affected by outliers
#package psych recenters the kurtosis values around 0, which is more common

library(psych)
kurtosi(rn1)
kurtosi(rn2)
kurtosi(rn.prod)
#similar to cauchy distribution, looks like normal dist
#but has very high levels of skewness

#CODING MISSING DATA-

x1<- c(1,2,3,NA,5)

summary(x1)
mean(x1) #returns NA

which(is.na(x1)) #give index number of na (which returns index)

#ignore missing value with na.rm=TRUE
mean(x1,na.rm=T)

#replace NA with a number (0 or some imputation - guesses which number to fill)
x2<-x1
x2[is.na(x2)]<-0
x2


#option 2: using ifelse - reminds of alteryx
x3<- ifelse(is.na(x1),0,x1)
x3

#Doing imputation (replacement via surrounding values): different packages
#mi: missing data imputation and model checking
#mice: multivariate imputation by chained equations
#imputation: --


#Challenge: Transform skewed data:
#xskew, variable x
#transform to remove outliers - dont delete
#ensure with hist/boxplot


xskew<- read.csv('C:/R-3.2.3/Play/Ex_Files_RStats_EssT/Exercise Files/Ch04/04_05_Challenge/xskew.csv')


x<-xskew$x
hist(x)
boxplot(x)
summary(x)

#squaring attempt - not perfect, but better. USE x^4: this is the video solution
x.sq<- x^2
hist(x.sq)
boxplot(x.sq)
boxplot.stats(x.sq)

#reflect plus transform
#Data that is negatively skewed requires a reflected transformation. This means that
#each data point must be reflected, and then transformed. To reflect a variable, create a
#new variable where the original value of the variable is subtracted from a constant. The
#constant is calculated by adding 1 to the largest value of the original variable. (E.g.
# (Largest value nL +1) - (original value nx). Next, transform the reflected data set. 
xr<- (max(x) +1)-x
#reflected set, now try log or sqrt

xr.ln<- log(xr)
summary(xr)
hist(xr.ln)
boxplot(xr.ln)
#SOOOOO CLOSE - 1 outlier with log

#CHAPTER 5: WORKING WITH DATA FILES ###################################################

mtcars<-mtcars

mean(mtcars$qsec[mtcars$cyl==8])
#mean qsec only for 8 cylinder cars

#mean mpg for cars above median hp
mean(mtcars$mpg[mtcars$hp>median(mtcars$hp)])

cyl.8<- mtcars[mtcars$cyl==8,]
#notice comma at end, to get all columns

mtcars[mtcars$cyl==8 & mtcars$carb>=4,]

#ANALYZING BY SUBGROUPS - AGGREGATE FUNCTION -----

iris<-iris

aggregate(iris$Petal.Width ~ iris$Species,FUN=mean)
#group by species, function applies to petal width

aggregate(cbind(iris$Petal.Width,iris$Petal.Length) ~
            iris$Species,FUN=mean)
#multiple variables at a time

#MERGING FILES########

data(longley)
longley

a1<- longley[1:14,1:6]
a2<- longley[1:14,6:7] # new column to add, with 'year' to match on

b<- longley[15:16,]

a12<- merge(a1,a2,by='Year')
#merge left to right

#merge rows, add observations
ab<- rbind(a12,b)
row.names(ab)<- NULL
ab

#CHAPTER 6: #####################################################
#CHARTS FOR ASSOCIATIONS ------------------------

require('datasets')
spray<- InsectSprays

spray
means<- aggregate(spray$count ~ spray$spray, FUN=mean)
means

mean.data<- t(means[-1]) # t = transpose, and remove first column
mean.data

colnames(mean.data)<- means[,1]

mean.data
barplot(mean.data)

#CREATING GROUPED BOXPLOTS----------------

require(MASS)
painters

data(painters)

boxplot(painters$Expression ~ painters$School)

#modified version
require('RColorBrewer')

#CREATING SCATTERPLOTS###########################
cars<-cars

plot(cars,
     pch=16,
     col='gray',
     xlab='speed',
     ylab='stopping distance')
#add regression line to the plot
abline(lm(cars$dist~cars$speed),
       col='darkred',
       lwd=2)
#locally weighted scatterplot smoothing
lines(lowess(cars$speed,cars$dist),
      col='blue',
      lwd=2)

install.packages('car')
library(car)
#scatteplot function has marginal boxplts, smoothers,
#and quantile regression intervals built in

scatterplot(cars$dist ~ cars$speed,
            pch=16,
            col='darkblue')

#CALCULATING CORRELATIONS--------------
swiss<-swiss
round(cor(swiss),2)

#can test one pair of variables at a time
#gives r, hypothesis test, and conf interval
cor.test(swiss$Fertility,swiss$Education)

#Hmisc package to get pvalues for matrix
install.packages('Hmisc')
library('Hmisc')

#must coerce dataframe to matrix
#to get corr matrix and pvalues
rcorr(as.matrix(swiss))

#COMPUTING A BIVARTIATE REGRESSION############

trees<-trees
trees

plot(trees$Girth,trees$Height)

#linear regression model
#corr is used to describe strength of association
#regression is used to allow you to predict 
#values of one variable based on values of another

reg1<- lm(Height ~ Girth,data=trees)
reg1
summary(reg1)
confint(reg1)
#95% conf interval

predict(reg1)
predict(reg1,interval='prediction') #CI for predicted height

#regression diagnostics
lm.influence(reg1)
influence.measures(reg1)


#COMPARING MEANS WITH the T-Test: 2 SAMPLE T TEST############

sleep<-sleep

sd<- sleep[,1:2]

#quick plots to check data
hist(sd$extra,col='lightgray')
boxplot(extra ~ group,data=sd)

#independent 2 group t=test (with defaults)
t.test(extra~group,data=sd)

t.test(extra~group,
       data=sd,
       alternative='less',
       conf.level=.80) # one tailed test, 80% CI(instead of 95%)

#create two groups of random data
#good because actual difference is known

x<- rnorm(30,mean=20,sd=5)
y<-rnorm(30,mean=22,sd=5)

t.test(x,y)

#COMPARING PAIRED MEANS: PAIRED T TEST###########
#compare 1 group before, and after

t1<- rnorm(50,mean=52,sd=6) #time1
dif<- rnorm(50,mean=6,sd=12) # Difference

t2<- t1+dif

t.test(t2,t1,paired=TRUE,
       mu=6,
       alternative='greater',
       conf.level=.99)

#COMPARING means with a one-factor ANOVA##########

x1<- rnorm(30,mean=40,sd=8)
x2<- rnorm(30,mean=41,sd=8)
x3<- rnorm(30,mean=45,sd=8)
x4<- rnorm(30,mean=46,sd=8)

xdf<- data.frame(cbind(x1,x2,x3,x4))
xdf
summary(xdf)

#stack data to get one column with outcome
#and second column with group
xs<-stack(xdf)

xs

anova1<- aov(values ~ ind,data=xs)
summary(anova1)

#posthoc comparisons
TukeyHSD(anova1)


#COMPARING PROPORTIONS ##############

n5<- c(rep(100,5))

x5<- c(65,60,60,50,45)
prop.test(x5,n5)
#statistically different 5 groups
#65/100 vs 60/100 vs 50/100 etc


#CREATING CROSS TABS FOR CATEGORICAL VARIABLES########

T<- Titanic

ftable(Titanic) #Flat table

#convert table to df with one row per observation
tdf<- as.data.frame(lapply(as.data.frame.table(Titanic),
                           function (x) rep(x,as.data.frame.table(Titanic)$Freq)))[,-5]

#create contingency table/crosstab
ttab<- table(tdf$Class,tdf$Survived)
ttab

round(prop.table(ttab,1),2) *100 # row %, 1 means to do rows
round(prop.table(ttab,2),2) *100 # column %

#CHI square test
tchi<- chisq.test(ttab)

#Computing Robust Statistics for bivariate associations ----------------------

install.packages('quantreg')
library(quantreg)

#will be doing quantile regression

data(engel)

attach(engel)
#attach allows shortcut references to data set columns

plot(income,
     foodexp,
     cex=.5)

points(income,
       foodexp,
       pch=16,
       col='lightgray')


taus<- c(.05,.1,.25,.75,.9,.95)
#quantiles

xx<- seq(min(income),max(income),100) #X values
f<- coef(rq((foodexp)~(income),tau=taus)) #coefficients
yy<-cbind(1,xx)%*%f #y values

for(i in 1:length(taus)){ # for each quantile value
  lines(xx,yy[,i], col='darkgray')     #draw regression line
}

abline(lm(foodexp ~ income), #Standard LS regression
       col='darkred',
       lwd=1)

abline(rq(foodexp ~ income),   #Median regression - much less affected by outlier
       col= 'blue',
       lwd =1)

legend(3000,1000,
       c('meanfit','medianfit'),
       col= c('darkred','blue'),
       lty=1,
       lwd=2)

detach(engel)


x<- c(31,57)
n<- c(72,96)

prop.test(x,n)



#CHAPTER 8: Charts for 3+ Variables #############

#Creating clustered bar chart for frequencies #######################

#load warpbreaks data
str(warpbreaks)

data<- tapply(warpbreaks$breaks,
              list(warpbreaks$wool, warpbreaks$tension),mean)   
#creates a 2x3 matrix 


barplot(data,
        beside=TRUE,
        col=c('steelblue3','thistle3'),
        bor=NA)


# for legend, locator(1) is interactive and lets you click where you want to put the legend.
#run this code and then click on plot to place legend.
legend(locator(1),
       rownames(data),
       fill=c('steelblue3','thistle3'))


#Creating scatterplots for grouped data ####################

str(iris)
install.packages('car')
library(car)



#single scatterplot with groups marked
#fxn can be called with sp or scatterplot
#chart width as a function of length, broken down by species#
sp(Sepal.Width ~ Sepal.Length | Species,
   data = iris,
   main = 'Iris Data',
   labels = row.names(iris))

#good way to show x,y relationship for 3 different groups on THE SAME SCATTER PLOT




#Creating a scatterplot Matrix #############################

data(iris)
str(iris)

attach(iris)

pairs(iris[1:4])
#chart first 4 variables - these are the quantitative variables

#Modified scatterplot matrices

#create palette with RColorBrewer

require('RColorBrewer')
display.brewer.pal(3,'Pastel1')
#displays colors which will be used


#put histograms on the diangol (see 'pairs' for help)
panel.hist<- function(x,...)
{
  usr<- par('usr');on.exit(par(usr))
  par(usr = c(usr[1:2],0,1.5))
  h<- hist(x,plot=FALSE)
  breaks<- h$breaks;nB<- length(breaks)
  y<- h$counts; y<- y/max(y)
  rect(breaks[-nB],0,breaks[-1],y, ...)
  #removed col = cyan from code block
}

pairs(iris[1:4],
      panel= panel.smooth,
      diag.panel = panel.hist,
      pch=16,
      col= brewer.pal(3,'Pastel1')[unclass(iris$Species)]) # this tells it to color according to the species
#looks much better


#similar with 'car' package
#gives kernel density and rugplot for each variable

library(car)
scatterplotMatrix(~Petal.Length + Petal.Width + Sepal.Length + Sepal.Width | Species,
                  data = iris,
                  col = brewer.pal(3,'Dark2'))


#CREATING 3d Scatterplots #########################

#spinning 3d scatterplot
#install and load rgl package
#however this is not compatible with R studio.


#CHAPTER 9: COMPUTING A MULTIPLE REGRESSION ######################


data(USJudgeRatings)

head(USJudgeRatings)

#FIRST IS OUTCOME VARIABLE, and then predictor variables after tilde
reg1<- lm(RTEN ~ CONT + INTG + DMNR+DILG+CFMG+DECI+PREP+FAMI+ORAL+WRIT+PHYS,
          data = USJudgeRatings)

reg1 # gives coeeficients only, join function
#negative coefficient in multiple regression may have a positive association if we tested it individually
#notice the coeficents change
lm(RTEN ~ CFMG + DILG, data = USJudgeRatings)

summary(reg1)
#gives p values with it
#adjusted R squared of .98, if we know the judges scores on these 11,  
#we can predict ninety eight or ninety nine percent of the variants on worthy of retention.
#The F-statistic, highly significant, so we have very, very, very good prediction from this model
#f stat is calculated by dividing SSBetween (diff between)/SSW (diff within groups). Large number means variation 
#between groups is due more to difference between groups
#need to use f-table to find CRITICAL F VALUE. IF F-Value > Critical Value, then SIGNIFICANT
# t-test for single variable, f-test for group of variables


#more detailed summaries
anova(reg1)

#possibility of stepwise variables selection
# (backwards and forwards); exercise caution!

#backwards stepwise regression
regb<- step(reg1,
            direction = 'backward',
            trace= 0) # dont print every step


summary(regb)


#forward stepwise regression
#start with model with constant and add variables


#COMPARING MEANS WITH 2-FACTOR ANOVA ###############
#allows you to use is two categorical predictor variables in a single quantitative outcome

data(warpbreaks)
head(warpbreaks)

#model with interaction
aov1<- aov(breaks ~
             wool + tension + wool:tension,
           # or: wool*tension (interaction of the two)
           data = warpbreaks)

summary(aov1)
model.tables(aov1,type='means')


TukeyHSD(aov1)



#CLUSTER ANALYSIS (KMEANS & HIERARCHICAL ANALYSES) ################

data(mtcars)
head(mtcars)

mtcars1<- mtcars[,c(1:4,6:7,9:11)]
#take a couple columns out

#three major kinds of clustering:
#   1- split into set number of clusters (kmeans)
#   2- hierarchical: start separate and combine
#   3- Dividing: start with single group and split

#we'll use hierarchical method
#Need distance matrix (dissimilarity Matrix)
#   eg. based on all of the variables in the dataset, miles per gallon and so on, 
#   you need to figure out how similar. Or how different each of the cars is from each of the cases. 

d<- dist(mtcars)

c<- hclust(d)
# use distance matrix for clustering , hclust = hier clust

plot(c)

#put observations in groups
#need to specify height = h or groups = k
g3<- cutree(c,k=3)


#or do several levels of groups at once
#'gm' = 'groups/multiple'
# shows which cluster each falls into, depending on k value
gm<- cutree(c,k=2:5) 
gm

#draw boxes around clusters
rect.hclust(c,k=3,border='tomato1')
rect.hclust(c,k=4,border='yellow')


#k-means clustering
km<- kmeans(mtcars1,3)
km

require(cluster)

clusplot(mtcars1,
         km$cluster,
         color=TRUE,
         lines=3,
         labels=2)


#CONDUCTING A PCA (PRINCIPAL COMPONENT ANALYSIS)/FACTOR ANALYSIS ###############

#difference b/w two from 'psych' package

#primary empirical difference between the components versus the factor model, 
#that's principal component versus factor analysis, 
#is the treatment of the variances for each item. That is in one of them, 
#it is assumed that the model explains the variances completely.
#Philosophically, components are weighted composites of observed factors. 
#That is the components themselves, the things that 
#you're abstracting are seen as a function of the observed variables.


data(mtcars)
mtcars1<- mtcars[,c(1:4,6:7,9:11)]


#PCA using default method
#if using entire df:
pc<- prcomp(mtcars1,
            center=TRUE, #centers mean to 0
            scale= TRUE) #sets unit variance
#or specify variable:
#pc<- prcomp(~mpg + cyl + disp, data = mtcars, scale = TRUE)

summary(pc)
#variables combined in a way, to create the different components
#shows how much variance is explained by each component


#screeplot
plot(pc)

pc
predict(pc)

biplot(pc)
